{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "cellView": "form",
        "id": "kDRmPT7DR_-u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3ad8877-acd2-4f97-e6b5-1eb8762117c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.10/dist-packages (2022.9.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (9.4.0)\n",
            "Requirement already satisfied: deepchem in /usr/local/lib/python3.10/dist-packages (2.7.2.dev20231121003347)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.12)\n",
            "Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.11.3)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (from deepchem) (2023.9.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2023.3.post1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->deepchem) (9.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepchem) (3.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->deepchem) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->deepchem) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "# @title Downloads RDkit, Deepchem & Transformers\n",
        "!pip install rdkit-pypi\n",
        "!pip install --pre deepchem\n",
        "!pip install transformers\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "vbfcpe_BT43l"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow import keras\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import deepchem\n",
        "from deepchem.feat.smiles_tokenizer import SmilesTokenizer\n",
        "from transformers import (BertConfig, BertForMaskedLM, BertModel,\n",
        "                          DataCollatorForLanguageModeling, Trainer, TrainingArguments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkRJj3W-UkGU",
        "outputId": "f5a0bc66-6a33-42bd-80a7-4c76f93d2bba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available: 1\n"
          ]
        }
      ],
      "source": [
        "# @title Check if GPU is available\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "c5xsv9v1WQVv"
      },
      "outputs": [],
      "source": [
        "# @title Canonical Smiles Function\n",
        "def get_canonical_smiles(smiles):\n",
        "    \"\"\"This function takes in a SMILES string and returns the canonicalized\"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    return Chem.MolToSmiles(mol, canonical=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "VtpL5nQVWY0J"
      },
      "outputs": [],
      "source": [
        "# @title Read in the data and preprocess\n",
        "csv_path = keras.utils.get_file(\n",
        "    \"/content/250k_rndm_zinc_drugs_clean_3.csv\",\n",
        "    \"https://raw.githubusercontent.com/aspuru-guzik-group/chemical_vae/master/models/zinc_properties/250k_rndm_zinc_drugs_clean_3.csv\",\n",
        ")\n",
        "\n",
        "data = pd.read_csv(csv_path)[:200]\n",
        "\n",
        "data.rename(columns={'SMILES': 'smiles'}, inplace=True)\n",
        "\n",
        "data = data[data[\"smiles\"].apply(lambda x: isinstance(x, str))]\n",
        "\n",
        "data['smiles'] = data['smiles'].apply(get_canonical_smiles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "klUhI042Zku6"
      },
      "outputs": [],
      "source": [
        "# @title Tokenizer\n",
        "if not os.path.exists('vocab.txt'):\n",
        "    !wget https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/vocab.txt\n",
        "\n",
        "tokenizer = SmilesTokenizer('vocab.txt')\n",
        "data['tokenized_smiles'] = data['smiles'].apply(tokenizer.encode)\n",
        "data = data[['smiles', 'tokenized_smiles', 'logP', 'qed', 'SAS']]\n",
        "data = data[data['tokenized_smiles'].apply(len) < 50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "eX0bvxHF-R4v"
      },
      "outputs": [],
      "source": [
        "# @title Padding\n",
        "def pad_sequence(seq):\n",
        "    return seq + [0] * (50 - len(seq))\n",
        "\n",
        "data['tokenized_smiles'] = data['tokenized_smiles'].apply(pad_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "6prTZKgPZtyJ"
      },
      "outputs": [],
      "source": [
        "# @title Add descriptors to data and normalize them\n",
        "selected_descriptors = [\n",
        "        'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3',\n",
        "        'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8',\n",
        "        'EState_VSA9', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12',\n",
        "        'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5',\n",
        "        'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10',\n",
        "        'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8',\n",
        "        'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2',\n",
        "        'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8',\n",
        "        'SlogP_VSA9', 'TPSA'\n",
        "    ]\n",
        "\n",
        "def compute_all_descriptors(smiles):\n",
        "    \"\"\"This function takes in a SMILES string and returns a dictionary of descriptors\"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    descriptor_names = [x[0] for x in Descriptors._descList[:124]]\n",
        "    descriptor_values = {}\n",
        "    for name in descriptor_names:\n",
        "        descriptor_func = getattr(Descriptors, name)\n",
        "        descriptor_values[name] = descriptor_func(mol)\n",
        "    return descriptor_values\n",
        "\n",
        "descriptors_df = data['smiles'].apply(compute_all_descriptors).apply(pd.Series)\n",
        "data = pd.concat([data, descriptors_df], axis=1)\n",
        "normalized_data = data.loc[:, 'logP':].apply(lambda x: (x-x.mean()) / x.std(), axis=0)\n",
        "normalized_data_merged = pd.merge(data[['smiles', 'tokenized_smiles']], normalized_data, right_index=True, left_index=True)\n",
        "normalized_data_merged.dropna(axis=1, inplace=True)\n",
        "descriptor_names = normalized_data_merged.columns.tolist()[2:]\n",
        "descriptor_names = list(set(descriptor_names))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "cellView": "form",
        "id": "4kLeebzszW-h"
      },
      "outputs": [],
      "source": [
        "# @title Define Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    \"\"\"This class defines the dataset for the masked language model\"\"\"\n",
        "    def __init__(self, data):\n",
        "        self.data = torch.tensor(data['tokenized_smiles'].to_numpy().tolist(), dtype=torch.long)\n",
        "        self.descriptors = torch.tensor(data[descriptor_names].to_numpy(), dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_ids = self.data[index]\n",
        "        attention_mask = (input_ids != 0).long()\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'descriptors': self.descriptors[index]\n",
        "        }\n",
        "\n",
        "\n",
        "train_data, temp_data = train_test_split(normalized_data_merged, test_size=0.8, random_state=42)\n",
        "validation_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataset = Dataset(train_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "validation_dataset = Dataset(validation_data)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "test_dataset = Dataset(test_data)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "cellView": "form",
        "id": "2DYIJBZNim2D"
      },
      "outputs": [],
      "source": [
        "# @title Define BERT model\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "config = BertConfig(\n",
        "  vocab_size=len(tokenizer.vocab),\n",
        "  hidden_size=768,\n",
        "  num_hidden_layers=12,\n",
        "  num_attention_heads=12,\n",
        "  intermediate_size=3072,\n",
        ")\n",
        "\n",
        "bert_model = BertModel(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "EVJPO_jzwzm_",
        "outputId": "fc891010-89ef-4253-9096-35ae63cd68a6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 00:03, Epoch 15/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Train MLM\n",
        "\n",
        "model = BertForMaskedLM(config=config).to('cuda')\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15\n",
        ")\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=15,\n",
        "    per_device_train_batch_size=32,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        ")\n",
        "trainer.train()\n",
        "model.save_pretrained(\"./results/bert_base\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "HFGbuS7Mz3w_"
      },
      "outputs": [],
      "source": [
        "# @title Multiheaded regression class\n",
        "class DescriptorHead(nn.Module):\n",
        "    \"\"\" This class defines the multiheaded regression heads\"\"\"\n",
        "    def __init__(self, input_dim=768, hidden_dim=64, output_dim=1):\n",
        "        super(DescriptorHead, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class BertForDescriptors(nn.Module):\n",
        "    \"\"\" This class defines the BERT model for predicting the descriptors and fingerprints \"\"\"\n",
        "    def __init__(self, num_descriptors=124):\n",
        "        super(BertForDescriptors, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"./results/bert_base\")\n",
        "        self.descriptor_heads = nn.ModuleList([DescriptorHead() for _ in range(num_descriptors)])\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Extract the last hidden state of the token `[CLS]` for the classification task\n",
        "        last_hidden_state_cls = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        descriptor_outputs = []\n",
        "\n",
        "        for head in self.descriptor_heads:\n",
        "            out = head(last_hidden_state_cls)\n",
        "            descriptor_outputs.append(out)\n",
        "\n",
        "        descriptor_outputs = torch.cat(descriptor_outputs, dim=1)\n",
        "\n",
        "        return descriptor_outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(descriptor_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HPnyQhfcaNY",
        "outputId": "ae2eb393-fb28-4cd3-c15f-fd1206455f18"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "122"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Tg5jWvG1TGe",
        "outputId": "79b57105-1a01-4864-e937-05d44d3a9590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at ./results/bert_base and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# @title Initialize model\n",
        "from torch.optim import Adam\n",
        "\n",
        "model = BertForDescriptors().to('cuda')\n",
        "optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.MSELoss(reduction='sum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmpwalN91Q6B",
        "outputId": "383ddba1-bc4f-428f-cfe6-69722b576b8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at ./results/bert_base and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed. Training Loss: 5470.9013671875, Validation Loss: 2927.000071207682\n",
            "Epoch 2 completed. Training Loss: 5448.3076171875, Validation Loss: 2913.324635823568\n",
            "Epoch 3 completed. Training Loss: 5373.34423828125, Validation Loss: 2900.9421997070312\n",
            "Epoch 4 completed. Training Loss: 5384.08935546875, Validation Loss: 2890.1082560221353\n",
            "Epoch 5 completed. Training Loss: 5426.20751953125, Validation Loss: 2880.4479166666665\n",
            "Epoch 6 completed. Training Loss: 5568.6318359375, Validation Loss: 2871.717081705729\n",
            "Epoch 7 completed. Training Loss: 5507.47998046875, Validation Loss: 2864.0065714518228\n",
            "Epoch 8 completed. Training Loss: 5321.64453125, Validation Loss: 2856.7686157226562\n",
            "Epoch 9 completed. Training Loss: 5442.1875, Validation Loss: 2850.515162150065\n",
            "Epoch 10 completed. Training Loss: 5294.7509765625, Validation Loss: 2844.8052927652993\n",
            "Epoch 11 completed. Training Loss: 5426.169921875, Validation Loss: 2839.8578033447266\n",
            "Epoch 12 completed. Training Loss: 5356.76171875, Validation Loss: 2835.2856648763022\n",
            "Epoch 13 completed. Training Loss: 5438.32763671875, Validation Loss: 2830.9471689860025\n",
            "Epoch 14 completed. Training Loss: 5288.86669921875, Validation Loss: 2826.9691772460938\n",
            "Epoch 15 completed. Training Loss: 5394.69140625, Validation Loss: 2823.6288859049478\n",
            "Epoch 16 completed. Training Loss: 5347.98095703125, Validation Loss: 2820.651336669922\n",
            "Epoch 17 completed. Training Loss: 5352.201171875, Validation Loss: 2817.9894409179688\n",
            "Epoch 18 completed. Training Loss: 5370.12451171875, Validation Loss: 2816.127614339193\n",
            "Epoch 19 completed. Training Loss: 5369.23046875, Validation Loss: 2813.7757161458335\n",
            "Epoch 20 completed. Training Loss: 5334.25390625, Validation Loss: 2811.885747273763\n"
          ]
        }
      ],
      "source": [
        "# @title Train model\n",
        "NUM_DESC = None\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to('cuda')\n",
        "        attention_mask = batch['attention_mask'].to('cuda')\n",
        "        labels = batch['descriptors'].to('cuda')\n",
        "\n",
        "        if NUM_DESC is None:\n",
        "            NUM_DESC = labels.shape[1]\n",
        "            model = BertForDescriptors(num_descriptors=NUM_DESC).to('cuda')\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_loader:\n",
        "            input_ids = batch['input_ids'].to('cuda')\n",
        "            attention_mask = batch['attention_mask'].to('cuda')\n",
        "            labels = batch['descriptors'].to('cuda')\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(validation_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} completed. Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "model.bert.save_pretrained(\"./results/bert_desc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "cellView": "form",
        "id": "xpVzhoJ0NgRN"
      },
      "outputs": [],
      "source": [
        "# @title Preprocessing\n",
        "def complete_preprocess(smiles, maxlen=50):\n",
        "    \"\"\" This function takes in a SMILES string and returns the tokenized and padded version of it, for new SMILES strings \"\"\"\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return None\n",
        "        canonical_smiles = Chem.MolToSmiles(mol, canonical=True)\n",
        "\n",
        "        tokenized = tokenizer.encode(canonical_smiles)\n",
        "\n",
        "        if len(tokenized) > maxlen:\n",
        "            return None\n",
        "\n",
        "        padded = pad_sequence(tokenized)\n",
        "\n",
        "        return padded\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "loaded_bert = BertModel.from_pretrained(\"./results/bert_desc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Kub4QUNKTy2Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4bb9057-b40c-42e6-cff4-a0de63bee8ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-58-39c985bbe803>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  attention_mask = (torch.tensor(padded_sequence) != 0).long()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.7163e-01,  2.5663e-01,  1.4859e-01, -1.3004e-01, -1.1941e+00,\n",
              "         -1.8084e+00,  2.5481e+00,  1.0384e+00,  5.6784e-01, -3.7080e-01,\n",
              "          2.2010e-01, -1.4087e-01,  3.2837e-01, -3.0373e-01, -8.1873e-01,\n",
              "          6.7028e-01,  8.8465e-01,  1.0756e+00, -1.0479e+00, -8.3670e-01,\n",
              "          6.4817e-01, -2.3405e+00,  1.0005e+00, -4.1317e-01,  6.7559e-01,\n",
              "          9.3714e-01,  8.5381e-01, -3.6162e-01, -1.3517e+00, -4.6039e-02,\n",
              "         -6.3393e-01, -4.4024e-01,  2.3152e-01,  5.3750e-01, -5.3529e-01,\n",
              "         -1.3979e+00,  2.2672e-01, -1.0644e-01, -1.2438e+00, -3.7595e-01,\n",
              "          1.4018e+00, -1.2716e+00,  5.2555e-01,  6.4929e-01, -9.1074e-01,\n",
              "         -5.3873e-02, -2.1571e-01,  9.9863e-01,  9.9201e-02, -7.5453e-01,\n",
              "          4.7636e-01,  3.1556e-01, -1.7149e+00,  1.3673e+00,  9.7640e-01,\n",
              "         -1.2437e+00, -2.8724e-01, -2.3562e-01,  1.3188e+00, -1.3816e+00,\n",
              "          8.4537e-01, -3.4560e-01,  9.0411e-01, -6.4129e-01, -3.6326e-01,\n",
              "          1.2738e+00,  2.0966e-01,  1.1278e+00,  8.9005e-01,  5.3194e-01,\n",
              "         -5.6039e-01,  1.0468e-01,  1.4336e+00, -2.5103e-01, -7.9383e-01,\n",
              "          2.1257e-01,  1.1588e+00,  6.0115e-01, -1.5945e-01,  3.0871e-01,\n",
              "         -6.3619e-01,  7.2465e-01, -8.1475e-01, -1.8443e+00, -3.3275e-02,\n",
              "         -2.4740e-01, -1.6699e+00, -2.4966e-01, -1.5611e+00,  8.4315e-01,\n",
              "          1.0079e-01, -6.8988e-02,  5.3629e-01, -3.3161e-01,  1.0981e+00,\n",
              "         -6.1832e-01, -1.4186e-01,  6.6700e-01, -2.2898e-01, -7.9361e-01,\n",
              "         -1.3258e+00, -1.5244e+00,  6.5037e-01, -7.3590e-01, -1.3278e-01,\n",
              "         -3.4845e-01, -8.8360e-01, -3.1770e-01,  3.3046e-01, -3.8984e-01,\n",
              "         -1.3832e+00,  3.4149e-01, -3.8239e-01, -7.2570e-01, -4.0615e-01,\n",
              "          2.6320e-01, -1.6949e+00, -1.9311e+00,  5.5703e-01,  1.5897e-01,\n",
              "         -7.7079e-01, -9.3166e-01,  1.9918e+00,  4.4586e-01, -1.8193e-02,\n",
              "          1.1204e+00, -1.6151e+00,  1.7862e+00,  2.8663e-01,  2.7819e+00,\n",
              "          1.1723e+00, -3.4954e-01,  1.3218e-01, -4.2032e-01,  6.1193e-01,\n",
              "         -6.0486e-01,  1.4307e+00, -1.2040e+00, -2.4111e-01,  2.1855e+00,\n",
              "          1.3883e+00,  1.3033e+00,  1.2344e+00,  7.2722e-01,  1.2623e+00,\n",
              "         -7.1525e-01, -4.5873e-01, -9.9379e-01,  6.4510e-01, -8.1099e-01,\n",
              "         -1.1698e+00, -3.7642e-01,  8.5457e-01, -4.3061e-01,  1.0837e+00,\n",
              "          5.4869e-01, -1.5929e+00,  2.8851e-01, -1.7800e+00,  1.3632e+00,\n",
              "          2.4418e-01, -6.8606e-01,  7.1479e-01,  6.5424e-01,  5.0281e-01,\n",
              "         -4.2261e-01,  1.9264e-01, -1.4743e+00, -1.2976e+00, -1.7719e+00,\n",
              "         -1.0284e-01, -1.1863e+00,  2.1838e-01,  7.5217e-01, -1.2624e+00,\n",
              "          2.7463e-01,  2.2641e-01,  2.2533e+00,  1.3110e+00,  1.0243e+00,\n",
              "         -1.5787e+00,  5.2512e-01, -3.0962e-02, -3.0723e-01, -4.4865e-01,\n",
              "          1.3295e+00,  6.2241e-01, -1.9540e+00, -1.3266e+00,  1.8916e+00,\n",
              "          7.0194e-02, -2.0194e-02,  8.7275e-01,  1.4164e+00,  2.3870e-03,\n",
              "          1.2930e+00, -1.9504e+00, -1.1401e+00,  6.7558e-01,  1.6589e-02,\n",
              "          2.4836e+00, -5.6221e-01,  1.4491e+00,  1.2564e+00,  1.0214e+00,\n",
              "          1.0846e+00,  1.8661e+00,  1.1765e+00, -1.8570e+00,  1.1740e-01,\n",
              "          6.2590e-01, -1.3318e+00,  6.8111e-01, -1.9211e-01,  1.5486e+00,\n",
              "         -4.8642e-01, -1.3353e+00, -1.9087e+00, -9.1237e-01, -2.0021e-01,\n",
              "         -1.8659e+00, -1.1393e+00,  4.9298e-01,  3.9206e-01,  2.1752e+00,\n",
              "         -1.0865e+00,  3.0257e-01, -9.9450e-01, -8.2306e-01, -4.4381e-01,\n",
              "         -2.6635e-01,  2.5358e-02,  1.2174e+00,  2.5172e-01, -2.5773e-04,\n",
              "         -1.3775e+00,  2.1062e+00, -1.4362e+00,  5.3179e-01, -5.0752e-01,\n",
              "          1.6891e+00, -7.0691e-01, -7.5145e-01,  3.0639e-01, -2.2075e-01,\n",
              "          9.8125e-01, -3.0237e-01, -2.2244e-01,  9.5392e-01, -1.7556e+00,\n",
              "          9.4763e-02, -1.3301e+00, -6.3795e-01, -4.2742e-02,  1.6149e+00,\n",
              "          8.4777e-01, -1.4429e+00, -9.2849e-01,  8.9072e-01,  6.8050e-01,\n",
              "          1.0909e+00, -9.7637e-01, -1.6815e-01, -1.8394e+00, -8.2981e-01,\n",
              "         -1.7670e+00,  8.7629e-02,  1.1027e-01, -2.4106e+00, -2.0222e+00,\n",
              "         -5.2911e-01,  2.0065e+00,  1.0984e+00,  5.3217e-01,  1.4108e+00,\n",
              "          1.8276e+00,  7.3942e-01,  4.2796e-01,  1.6783e+00,  9.4122e-02,\n",
              "          1.5610e-01,  2.1356e-04, -1.0674e+00, -2.2548e-01, -5.4013e-02,\n",
              "          4.1282e-01, -1.1777e-01, -1.4015e+00,  1.8247e-01, -6.1834e-01,\n",
              "         -1.8889e-01, -1.0859e+00,  2.9228e-01,  1.8934e+00, -9.7187e-01,\n",
              "         -3.4585e-01, -3.7882e-01, -3.6636e-01, -4.2288e-01,  6.5431e-01,\n",
              "          7.7774e-01,  5.2789e-01,  1.4704e+00, -8.0581e-02, -4.8173e-01,\n",
              "          1.4771e-01,  6.8112e-03,  9.9534e-01,  1.5178e-01,  1.1739e+00,\n",
              "          9.4015e-01,  1.4108e+00, -1.9377e-02,  2.3722e-01, -1.8640e-01,\n",
              "          4.1687e-01,  1.0772e-01,  3.0948e-02, -1.2173e+00,  3.2962e-01,\n",
              "          1.6229e+00, -1.1441e-01,  4.9396e-01,  8.6323e-01,  5.9076e-01,\n",
              "         -3.3743e-01,  4.2236e-01, -6.2910e-01, -1.8088e+00,  1.7806e+00,\n",
              "          3.7032e-01, -3.1880e-01, -4.8259e-01,  1.5666e-01,  1.0551e+00,\n",
              "          1.5567e+00, -7.0780e-01, -6.4364e-01,  7.2538e-01, -8.4508e-01,\n",
              "         -3.6120e-01, -2.7672e-01, -7.6648e-01, -1.5959e-02, -5.5277e-01,\n",
              "         -1.4179e+00, -3.4430e-01, -3.3251e-01,  3.6866e-01, -2.2810e-01,\n",
              "         -7.6163e-01, -2.0771e-01,  3.9039e-01, -1.5154e-01, -9.6446e-01,\n",
              "          1.3129e-01,  1.1428e+00, -1.4005e+00,  1.3249e+00, -1.3633e+00,\n",
              "         -6.9370e-01, -6.6518e-01, -2.4697e+00,  4.9585e-01,  5.3936e-01,\n",
              "          9.9570e-01,  7.6612e-01, -6.1309e-01,  5.9276e-01,  4.7737e-01,\n",
              "          9.6057e-01, -4.9268e-01, -1.1043e-01,  1.0811e+00, -8.1329e-01,\n",
              "         -1.4737e+00, -3.3934e-01,  5.7437e-02,  2.8340e-01,  1.2682e-01,\n",
              "         -2.9856e-01, -1.1342e+00, -8.4080e-01, -3.9507e-01,  7.9805e-01,\n",
              "          1.4309e-02,  6.1871e-02, -3.6455e-01, -1.5155e+00,  7.7339e-01,\n",
              "          9.1247e-01,  2.9023e-02,  1.2736e+00, -8.9927e-01,  2.1841e+00,\n",
              "         -2.6651e-01, -2.7186e-01, -1.5716e+00,  6.9608e-01,  6.8714e-01,\n",
              "          3.9204e-01,  7.8964e-01, -1.7839e+00,  9.6756e-01,  3.0841e+00,\n",
              "         -8.0131e-01,  9.4351e-01, -1.2248e+00,  1.9332e+00, -1.1295e+00,\n",
              "         -5.1094e-01,  3.9513e-01, -4.7952e-01, -6.3024e-01, -2.4190e+00,\n",
              "          1.4601e+00,  8.1538e-01,  6.4486e-01,  1.2182e+00, -6.6724e-01,\n",
              "         -1.4661e+00, -5.3647e-01, -8.9894e-01, -1.0967e+00, -4.1933e-01,\n",
              "          6.4972e-02, -6.3042e-01, -1.0092e+00, -1.0338e+00, -1.2539e+00,\n",
              "         -3.0654e-01, -5.0408e-01,  1.6526e-01, -9.5714e-01, -7.6705e-01,\n",
              "          1.1058e+00, -9.1856e-01, -1.7582e+00,  1.4871e+00, -6.9520e-01,\n",
              "          4.4542e-01,  5.4391e-01, -8.2783e-01,  4.4246e-01,  1.0154e-01,\n",
              "         -3.6689e-01,  5.5347e-01,  8.4148e-01, -3.2889e-01, -4.8927e-01,\n",
              "          1.0961e+00, -8.0011e-01, -1.6691e+00,  2.7824e-01,  9.3991e-01,\n",
              "          3.0510e-01,  1.3886e+00, -9.8275e-01, -5.7980e-01,  5.7529e-01,\n",
              "          1.1980e-01,  2.0319e-01,  1.5865e-01, -1.3736e+00, -2.9374e-01,\n",
              "          1.5816e+00,  1.3388e+00, -1.0777e+00,  1.1253e+00, -2.6778e-01,\n",
              "         -5.8320e-01,  2.9646e-01, -8.2511e-01,  1.0892e+00,  2.1233e-01,\n",
              "         -1.9918e+00,  8.3444e-01,  1.5718e+00,  7.9765e-01, -3.3649e-01,\n",
              "         -2.4116e+00,  1.8356e+00, -1.5261e+00, -4.8818e-01,  2.1010e+00,\n",
              "         -2.2279e-01, -1.4593e+00, -7.8891e-01, -4.2069e-01, -1.0366e+00,\n",
              "         -2.0482e-01, -2.1368e-01, -1.8188e+00,  3.6963e-01,  1.0268e+00,\n",
              "         -5.0255e-01,  6.9100e-01,  1.0455e-01, -4.5129e-01, -8.4500e-01,\n",
              "         -7.0446e-01,  7.3128e-01, -2.2728e-01,  9.3531e-01,  7.2983e-01,\n",
              "          1.4648e+00, -7.1804e-01, -2.8587e-01, -1.2304e+00, -1.2167e+00,\n",
              "          1.0020e+00,  1.0230e+00, -1.9480e+00,  3.2874e-01,  1.9841e-01,\n",
              "          6.7217e-01,  1.0127e-01, -1.8079e-01,  6.0324e-02,  2.3061e-02,\n",
              "          1.7139e-01,  3.4188e-01, -6.5395e-01,  1.9339e+00,  1.1811e+00,\n",
              "         -7.4323e-01, -1.1760e+00,  1.6852e+00, -6.4620e-01, -1.8904e-01,\n",
              "          2.5003e-01, -2.9253e-02, -2.4337e+00,  5.0497e-01, -1.8020e+00,\n",
              "         -9.2162e-01,  4.2332e-01, -5.6072e-01, -4.9575e-01, -3.7741e-01,\n",
              "          3.4416e-01, -7.6736e-01,  3.5876e-02,  1.6261e+00,  7.7538e-01,\n",
              "          2.4606e-01, -7.7085e-01, -2.2596e-01,  8.6744e-01, -1.1113e+00,\n",
              "          1.4891e+00, -9.7496e-01,  1.3111e+00, -1.4798e-01,  2.5347e-01,\n",
              "          2.0684e+00,  3.2258e-01, -2.2848e-01,  3.3967e-02,  6.0770e-01,\n",
              "          2.6747e+00, -1.1419e+00, -1.3242e+00, -6.6655e-01, -6.1920e-01,\n",
              "          6.1940e-03,  4.1221e-01,  2.0115e+00, -6.3264e-01,  2.0203e-01,\n",
              "          1.3216e+00, -5.9983e-01,  5.1814e-01, -3.4325e-01, -7.9373e-01,\n",
              "         -8.4588e-03, -5.8639e-01,  1.3753e+00,  9.9028e-01,  6.9789e-01,\n",
              "         -1.2768e+00,  6.0727e-01,  2.2787e-01, -1.2795e-01,  9.3822e-01,\n",
              "         -1.3028e+00, -1.4160e+00, -4.3392e-01, -7.3207e-01, -5.7057e-02,\n",
              "         -1.0448e-01,  9.8205e-01, -5.1895e-02,  1.0623e+00, -6.7513e-01,\n",
              "         -1.5557e+00,  1.3270e+00, -1.3302e+00,  1.8439e+00, -1.2723e+00,\n",
              "         -8.3184e-01,  1.0913e+00,  1.3135e+00,  5.5159e-01,  7.8073e-02,\n",
              "          7.3649e-01,  1.2073e-02,  1.8606e+00,  1.5743e+00,  4.3760e-01,\n",
              "         -7.6435e-01, -2.3541e+00,  2.3022e-02,  4.3613e-01,  6.8005e-01,\n",
              "         -4.1047e-01,  9.3178e-01, -1.7565e+00, -1.8668e+00,  1.5915e+00,\n",
              "         -1.5615e+00, -2.2088e+00, -2.4133e+00, -1.0473e+00,  1.8613e+00,\n",
              "          8.9937e-01, -4.4553e-01, -1.2461e+00, -1.9910e-01, -7.7714e-02,\n",
              "         -1.5849e-01, -8.2794e-01,  1.1214e+00, -1.6283e-01,  4.9241e-01,\n",
              "         -1.4727e+00, -2.1251e-01,  3.2520e-01, -9.7831e-02, -1.0574e+00,\n",
              "         -9.1471e-01, -9.5702e-01, -2.7567e-01, -7.2753e-02, -3.2020e-01,\n",
              "          1.3522e+00,  5.6685e-01,  3.1500e-01, -7.0819e-01, -1.1933e+00,\n",
              "          1.6270e+00,  3.3118e-01,  1.5940e+00, -1.6813e+00, -3.3974e-01,\n",
              "          1.5950e-01,  9.3625e-01, -1.1190e+00,  3.3122e-02, -6.2704e-02,\n",
              "          4.2874e-01, -1.0264e+00,  1.3892e+00, -9.6034e-01, -1.6296e+00,\n",
              "          1.0430e+00, -1.1289e+00,  1.2590e+00,  3.2376e-01, -1.2930e-01,\n",
              "         -1.4835e+00, -9.1566e-01,  9.9745e-01, -1.2026e-01,  8.9227e-01,\n",
              "          6.7451e-01,  3.7696e-01, -6.5247e-01,  1.0692e+00, -5.9272e-01,\n",
              "         -8.7971e-01,  1.0753e+00, -1.2063e-01, -1.4743e+00, -1.3464e+00,\n",
              "          1.4852e+00,  1.0291e+00, -1.6489e+00,  9.4985e-02, -4.8123e-01,\n",
              "         -9.5446e-01, -2.7430e-01, -2.1940e-01,  1.0648e+00,  8.4589e-01,\n",
              "          9.2172e-01,  2.8234e-01,  8.4867e-01,  1.7561e-01,  3.6415e-01,\n",
              "         -7.6889e-01,  1.5858e+00,  1.4845e+00, -9.5746e-01, -6.7109e-01,\n",
              "         -1.5039e+00, -5.1261e-01,  1.0931e+00,  1.0574e+00,  8.5932e-01,\n",
              "          7.3077e-01,  6.2039e-01,  4.7457e-01, -6.3357e-01,  6.7630e-01,\n",
              "          4.8278e-01, -4.1413e-01,  2.6808e-02,  3.7951e-01,  7.9509e-01,\n",
              "         -1.7904e+00, -8.9639e-01, -1.7006e-01, -1.5682e+00,  1.3344e+00,\n",
              "          1.0241e+00,  1.7202e+00,  4.5538e-03,  1.5676e+00,  1.2647e+00,\n",
              "          1.4574e+00,  8.0305e-01, -1.4916e+00, -5.2701e-01,  1.0939e-01,\n",
              "          1.0411e+00,  8.3543e-01, -6.0587e-01, -4.3769e-01, -1.2399e+00,\n",
              "         -4.5935e-01,  1.0321e+00, -1.3425e+00,  5.5866e-01, -2.6200e-01,\n",
              "          1.6435e+00,  1.1968e+00,  1.6904e-01, -7.7759e-01,  4.6222e-01,\n",
              "         -9.8021e-01,  5.9722e-01, -3.6203e-01, -7.8092e-01, -1.1387e+00,\n",
              "         -1.4577e+00, -2.4417e-01,  5.5477e-01,  2.0495e+00,  1.2823e+00,\n",
              "          1.6463e+00,  1.3325e-01,  9.0423e-02,  4.0606e-01, -1.6528e-03,\n",
              "          2.3031e+00, -1.6238e+00,  5.1390e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "# @title Get Fingerprint\n",
        "def get_representation(smiles, model):\n",
        "    \"\"\" This function takes in a SMILES string and returns the representation of the `[CLS]` token / fingerprint \"\"\"\n",
        "    try:\n",
        "        preprocessed_data = complete_preprocess(smiles)\n",
        "        if preprocessed_data is None:\n",
        "            return None\n",
        "\n",
        "        padded_sequence = torch.tensor([preprocessed_data])\n",
        "\n",
        "        attention_mask = (torch.tensor(padded_sequence) != 0).long()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = model(padded_sequence, attention_mask)\n",
        "\n",
        "        # Extract the [CLS] token's features\n",
        "        cls_features = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        return cls_features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "get_representation('COC', loaded_bert)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}